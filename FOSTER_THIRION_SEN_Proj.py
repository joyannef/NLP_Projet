#  ...know a word by the company it keeps...
# This programme is a basic algorithm based on Bayes theorem of probability,
# and aims for automatic generation of sentences based on several wine reviews

# P.S. in the 'generate' function we can choose the number of reviews we want to generate 

# Exercise.1
# a.
# P(A|B) = P(AB)/P(B)
# As the Bayes formula indicates, we can calculate the probability of knowing a word from the two previous words with the following logic:
# P(A|B) would represent the probability of knowing a word given the words before it eg. (P(like|w1, w2....wn)).
# This is the probability of the number of times "like" appears given other words in a sentence.
# In this sense, A = like and B = the other words
# We then need to multiply the probabilities of A and B and divide this result by P(B) 
# in order to calculate the probablity of knowing a wi given the two words prior (wi-1, wi-2).



import pprint as pp
from collections import defaultdict
import numpy as np
from nltk.tokenize import word_tokenize
import string

wordList = []
tupList = []
original = ()
occurence = defaultdict(list)


## b.
# creates trigrams for each review, that begins with BEGIN NOW and END
# appending to a list of tuples
# another alternative would be nltk's trigram function 
def make_trigrams(sentStr):
    tri_length = 3
    i = 0
    wordList = word_tokenize(sentStr)
    
    while i < len(wordList) - 2:
        templine = wordList[i: (i + tri_length)]
        tupList.append(tuple(templine))
        i += 1

    return tupList


# #c.
# function that calculates the number of possible words that follow a set of two words or bigram,
# which in this case is the first two words of each tuple
# 1st part takes each tuple and appends to python dictionary : {(bigram) : {w1: n1_times, w2: n2_times, w3: n3_times....}}
# the second part, turns the counts into probability (a number between 0 and 1) {(bigram) : {w1: proba1, w2: proba2, w3: proba3....}}

def make_conditional_probas(tupList):
    
    for tup in tupList:
        new_key = tup[0:2]
        new_value = {tup[2]: 1}
        if new_key in occurence.keys():
            check_key = occurence[new_key]
            if tup[2] == check_key.keys():
                # then increase existing value by 1, else update value a new set of key:value
                new_value[tup[2]] += 1
                #print(new_value)
            else:                    
                new_value.update(check_key)
        occurence[new_key] = new_value

    
    for k, v in occurence.items():
        # total occurence of bi_gram = dividor
        dividor = (sum(v.values()))
        for ele, val in v.items():
            v[ele] = (v[ele]/ dividor)   
            # not rounding the above result, else risk of triggering zero division error, 
            # (especially) while calling function : sample_from_discrete_distrib
    
    return(occurence)   


##d.
# we have decided to base our study on generating word wi based on two previous words wi-2 and wi-3
# (BEGIN, NOW) is the bigram that demarcates the beginning of a review.
# They have been added so that every time we find it, we can look for other possible words that can follow.
# Also, from a technical point of view, it's a tuple, so we know that it cannot be accidentally modified, & needs 2 items minimum 



# function : takes as arguement a dictionary (which is the value of a bigram) 
# generates and returns random words based on the probability calculation 
# a word which has a higher probability, has a higher chance of being picked 
def sample_from_discrete_distrib(distrib):
    words, probas = zip(*distrib.items())
    probas = np.asarray(probas).astype('float64')/np.sum(probas)
    return np.random.choice(words, p=probas)

## 2.1 
# initalise history by (BEGIN NOW), since we know that reviews begin with it
# generate looks for (BEGIN NOW) and sends its value (also a dictionary) to sample_from_discrete_distrib to generate random words
# once it finds END, it resets initial tuple to (BEGIN NOW) and repeats the process
def generate (occurence):
    print_list = []
    # here we decide how many sentences we want to generate
    num_sentences = 15
    i = 0
    while i < num_sentences:
        original = ('BEGIN', 'NOW')
        print_list.append(original[0] + " "  + original[1])
        while original[1] != 'END':
                add_item = sample_from_discrete_distrib(occurence[original])
                temp = original[1]
                original = (temp, add_item)
                print_list.append(add_item)

        # to get a new line after each "END" of sentence
        print_list.append("\n")  
        i += 1    

    return print_list



## 1. main function
# opens file, to read lines
# calls various functions in order to get a list of sentences generated by probability algorithm
with open("wine2.txt") as file:
    rows = file.readlines()

    # tuplist is a trigrams for each line delimited by BEGIN NOW and END
    for row in rows:
        tupList = make_trigrams(row)
     
    # occurence is a dictionary that gets conditional probabilities with function make_conditional_probas
    occurence = make_conditional_probas(tupList)
    # in case we needed to have a look at the dictionary:
    ##pp.pprint(occurence)
    
     # function generate creates a list of tokens / reviews that will be the generated by the algorithm.
    sentence_list = generate(occurence)


    # cleaning up the sentences before printing
    # sticking the punctuations marks to the word that precedes it, as is the rule in English
    # needs further verification to catch possible errors  
    length = len(sentence_list) -1
    print(length)
    element = 0
    while element < length :
        if sentence_list[element] in string.punctuation: 
            sentence_list[element-1] = sentence_list[element-1] + sentence_list[element]
            del sentence_list[element]
            
            length -= 1
        element += 1   
    print(' '.join(sentence_list))
            

# 2.2, 2.3
# The sentences obtained are surprisingly neat and creative.
# Sometimes those that precede END are truncated
# We could further improve,by removing the unfinished sentences, or creating an algorithm to finish them!

